{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Model Tutorial\n",
    "This notebook give a tutorial on adding a base model that supports benchmark tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model API and Considerations\n",
    "1. The model needs to implement the interface `forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec)`, which is forward propagation, and the meaning of each parameter is as follows:\n",
    "   * input:\n",
    "     * `x_enc`: the input encoder sequence data\n",
    "     * `x_mark_enc`: marks associated with the encoder sequence data, such as timestamps, category labels, etc.\n",
    "     * `x_dec`: Input sequence data of the decoder\n",
    "     * `x_mark_dec`: decoder sequence data related markers\n",
    "   * output:\n",
    "     * the return is a 3D tensor with dimensions [batch_size, pred_len, feature_num].\n",
    "2. model initialization (\\_\\_init\\_\\_), accept related configuration parameters, configuration parameters can be used in the model, see example for details.\n",
    "3. model class must inherit from `torch.nn.Module`, otherwise it cannot be accessed by the optimizer.\n",
    "4. In the \\_\\_init\\_\\_ function of the `exp/exp_basic.py` file, add your model name in the dictionary, so that the framework reads your model.\n",
    "\n",
    "One modeling framework is roughly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        # other configurations\n",
    "        # model initialization\n",
    "        # ......\n",
    "        \n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # encoder and decoder\n",
    "        # ......\n",
    "        pass\n",
    "        # return dec_out\n",
    "        \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # depending on the task name, you can choose different ways of forward propagation, here is the prediction task\n",
    "        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the iTransformer model as a proven example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers.Transformer_EncDec import Encoder, EncoderLayer\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Embed import DataEmbedding_inverted\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper link: https://arxiv.org/abs/2310.06625\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.task_name = configs.task_name\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,\n",
    "                                                    configs.dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=False), configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.projection = nn.Linear(configs.d_model, configs.pred_len, bias=True)\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        _, _, N = x_enc.shape\n",
    "\n",
    "        # Embedding\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "\n",
    "        dec_out = self.projection(enc_out).permute(0, 2, 1)[:, :, :N]\n",
    "        # De-Normalization from Non-stationary Transformer\n",
    "        dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "        dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
